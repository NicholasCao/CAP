{
  "adam_epsilon": 1e-08,
  "data_dir": "data/superglue/rte",
  "do_eval": true,
  "do_train": true,
  "eval_batch_size": 8,
  "eval_every_step": 50,
  "gradient_accumulation_steps": 8,
  "hidden_size": 256,
  "learning_rate": 2e-05,
  "max_grad_norm": 1.0,
  "max_seq_length": 256,
  "model_name_or_path": "bert-large-cased",
  "model_type": "bert",
  "no_cuda": false,
  "num_train_epochs": 5,
  "pattern_id": 2,
  "seed": 1024,
  "special_prompt": true,
  "task_name": "rte",
  "train_batch_size": 16,
  "warmup_steps": 80,
  "weight_decay": 0.01,
  "wrapper_type": "mlm",
  "zero_shot": true
}