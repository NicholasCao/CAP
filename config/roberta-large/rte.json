{
  "data_dir": "data/superglue/rte",
  "gradient_accumulation_steps": 8,

  "seed": 1024,
  "model_type": "roberta",
  "model_name_or_path": "roberta-large",
  "wrapper_type": "mlm",

  "unique_prompt": true,
  "pattern_id": 2,

  "task_name": "rte",
  "num_train_epochs": 5,
  "train_batch_size": 16,
  "eval_batch_size": 8,
  "learning_rate": 2e-5,
  "warmup_steps": 80,
  "max_grad_norm": 1.0,
  "adam_epsilon": 1e-8,
  "weight_decay": 0.01,
  "max_seq_length": 256,

  "no_cuda": false,

  "do_train": true,
  "do_eval": true,
  "eval_every_step": 50,

  "hidden_size": 256,
  "zero_shot": true
}
